---
title: "MUSA 508, Lab 4 - Spatial Machine Learning Pt. 1"
author: "Harris, Fichman, and Steif - 2022/23"
output: html_document
---

```{r setup, include=FALSE}

# You can set some global options for knitting chunks

knitr::opts_chunk$set(echo = TRUE)

# Load some libraries

library(tidyverse)
library(tidycensus)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(broom)
library(tufte)
library(rmarkdown)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots

# functions and data directory
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

palette5 <- c("#25CB10", "#5AB60C", "#8FA108",   "#C48C04", "#FA7800")
```

# MUSA 508, Lab 4 - Spatial Machine Learning Pt. 1

The learning objectives of this lab are:

*   Review Topics: loading data, {dplyr}, mapping and plotting with {ggplot2}
*   New: Understanding how we created spatial indicators with K-Nearest Neighbor and the `nn_function()` function
*   Simple correlation and the Pearson's r - Correlation Coefficient
*   Linear Regression model goodness-of-fit and R2 - Coefficient of Determination

The in-class exercise at the end encourages students to modify the existing code to create a different regression and interpret it. Finally, there is a prompt to create a new feature and add it to the regression.

## Data Wrangling

See if you can change the chunk options to get rid of the text outputs

```{r load_key, warning = FALSE, eval = FALSE}
census_api_key("5c7b1ebb206012789759942ddf1acbb882f937ad", overwrite = TRUE)
```

```{r acs_vars}
acs_vars <- c("B01001_001E", # ACS total Pop estimate
              "B25002_001E", # Estimate of total housing units
              "B25002_003E", # Number of vacant housing units
              "B19013_001E", # Median HH Income ($)
              "B02001_002E", # People describing themselves as "white alone"
              "B06009_006E") # Total graduate or professional degree
```

```{r get_acs_2020, cache = TRUE, message = FALSE, warning = FALSE}
acsTractsPHL.2020 <- get_acs(geography = "tract",
                             year = 2020, 
                             variables = acs_vars, 
                             geometry = TRUE, 
                             state = "PA", 
                             county = "Philadelphia", 
                             output = "wide") %>%
                            st_transform('ESRI:102729')
```

```{r read_data}

nhoods <- 
  st_read("D:/Upenn/23fall/MUSA5080 Public Policy Analysis/midterm 4-5/studentData.geojson") %>%
  st_transform('ESRI:102729')

to_predict <-
  nhoods %>%
  dplyr::filter(toPredict == 'CHALLENGE') #Select data for later prediction

to_train <-
  nhoods %>%
  dplyr::filter(toPredict == 'MODELLING') #Select data for training Model

#??
#Philadelphia <- 
 # read.csv(file.path(root.dir,"/Chapter3_4/phillyHousePriceData_clean.csv"))


# Load crime data
philadelphiCrimes <- read.csv('https://raw.githubusercontent.com/ObjQIAN/MUSA-508-Midterm/main/data/Philadelphia_crime.csv') 

# Create sf and select Weapon Violations
philadelphiCrimes.sf <-
  philadelphiCrimes %>%
  filter(text_general_code == "Weapon Violations",
  lat > -1) %>%
  dplyr::select(lat, lng) %>%
  na.omit() %>%
  st_as_sf(coords = c( "lng","lat"), crs = "EPSG:4326") %>%
  st_transform('ESRI:102729') %>%
  distinct()




# Load 311 data
philadelphia311 <- read.csv('https://phl.carto.com/api/v2/sql?filename=public_cases_fc&format=csv&skipfields=cartodb_id,the_geom,the_geom_webmercator&q=SELECT%20*%20FROM%20public_cases_fc%20WHERE%20requested_datetime%20%3E=%20%272022-01-01%27%20AND%20requested_datetime%20%3C%20%272023-01-01%27') %>%
  dplyr::select(lat, lon) %>%
  na.omit() %>%
  st_as_sf(coords = c("lon", "lat"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102729')



```



```{r price_map}
# ggplot, reorder

# Mapping data
ggplot() +
  geom_sf(data = acsTractsPHL.2020, fill = "grey40") +
  geom_sf(data = to_train, aes(colour = q5(sale_price)), 
          show.legend = "point", size = .35) +
  scale_colour_manual(values = palette5,
                   labels=qBr(to_train,"sale_price"),
                   name="Quintile\nBreaks") +
  labs(title="House Sale Price, Philadelphia") +
  mapTheme()
```


### Create Nearest Spatial Features
This is where we do two important things:

*   aggregate values within a buffer, and 
*   create  `nearest neighbor` features. 

These are primary tools we use to add the local spatial signal to each of the points/rows/observations we are modeling. 

#### Buffer aggregate

The first code in this block buffers each point in `nhoods` by `660` ft and then uses `aggregate` to count the number of `phillyCrimes.sf` points within that buffer. There is a nested call to `mutate` to assign a value of `1` for each `phillyCrimes.sf` as under a column called `counter`. This allows each crime to have the same weight in when the `sun` function is called, but other weighting schemes could be used. Finally, the code `pull` pulls the aggregate values so they can be assigned as the `crimes.Buffer` column to `nhoods`. This is a little different from how you had assigned new columns before (usually using `mutate`), but valid. Your new feature `crimes.Buffer` is a count of all the crimes within 660ft of each reported crime. Why would it be god to know this when building a model?  


#### k nearest neighbor (knn)

The second block of code is using knn for averaging or summing values of a set number of (referred to as `k`) the nearest observations to each observation; it's *nearest neighbors*. With this, the model can understand the magnitude of values that are *near* each point. This adds a spatial signal to the model. 

<!-- The function `nn_function()` takes two pairs of **coordinates** form two `sf` **point** objects. You will see the use of `st_c()` which is just a shorthand way to get the coordinates with the `st_coordinates()` function. You can see where `st_c()` is created. Using `st_c()` within `mutate` converts the points to longitude/latitude coordinate pairs for the `nn_function()` to work with. If you put *polygon* features in there, it will error. Instead you can use `st_c(st_centroid(YourPolygonSfObject))` to get nearest neighbors to a polygon centroid. -->

The number `k` in the `nn_function()` function is the number of neighbors to to values from that are then averaged. Different types of crime (or anything else you measure) will require different values of `k`. *You will have to think about this!*. What could be the importance of `k` when you are making knn features for a violent crime versus and nuisance crime?

```{r Features}

# Counts of crime per buffer of house sale
to_train$crimes.Buffer <- to_train %>% 
    st_buffer(660) %>% 
    aggregate(mutate(philadelphiCrimes.sf, counter = 1),., sum) %>%
    pull(counter)


## Nearest Neighbor Feature

to_train <-
  to_train %>% 
    mutate(
      crime_nn1 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphiCrimes.sf), k = 1),
      
      crime_nn2 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphiCrimes.sf), k = 2), 
      
      crime_nn3 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphiCrimes.sf), k = 3), 
      
      crime_nn4 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphiCrimes.sf), k = 4), 
      
      crime_nn5 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphiCrimes.sf), k = 5)) 
```

```{r assault density}
## Plot assault density
ggplot() + geom_sf(data = acsTractsPHL.2020, fill = "grey40") +
  stat_density2d(data = data.frame(st_coordinates(philadelphiCrimes.sf)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "#25CB10", high = "#FA7800", name = "Density") +
  scale_alpha(range = c(0.00, 0.35), guide = "none") +
  labs(title = "Density of WA, PHL") +
  mapTheme()
```

## Analyzing associations

Run these code blocks...
Notice the use of `st_drop_geometry()`, this is the correct way to go from a `sf` spatial dataframe to a regular dataframe with no spatial component.

Can somebody walk me through what they do?

Can you give me a one-sentence description of what the takeaway is?

```{r Correlation}

## Home Features cor
st_drop_geometry(to_train) %>% 
  mutate(Age = 2022 - year_built) %>%
  dplyr::select(sale_price, total_livable_area, Age, crimes.Buffer) %>%
  filter(sale_price <= 1000000, Age < 500, total_livable_area <10000) %>%
  gather(Variable, Value, -sale_price) %>% 
   ggplot(aes(Value, sale_price)) +
     geom_point(size = .5) + 
    geom_smooth(data = . %>% filter(sale_price >0), method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, ncol = 3, scales = "free") +
     labs(title = "Price as a function of continuous variables") +
     plotTheme()
```

#定性变量的分类需要考虑（style等）
to_train %>% 
  dplyr::select(sale_price, Style, OWN_OCC, NUM_FLOORS) %>%
  mutate(NUM_FLOORS = as.factor(NUM_FLOORS)) %>%
  filter(SalePrice <= 1000000) %>%
  gather(Variable, Value, -SalePrice) %>% 
   ggplot(aes(Value, SalePrice)) +
     geom_bar(position = "dodge", stat = "summary", fun.y = "mean") +
     facet_wrap(~Variable, ncol = 1, scales = "free") +
     labs(title = "Price as a function of\ncategorical variables", y = "Mean_Price") +
     plotTheme() + theme(axis.text.x = element_text(angle = 45, hjust = 1))


Can we resize this somehow to make it look better?

```{r crime_corr}
## Crime cor
to_train %>%
  st_drop_geometry() %>%
  mutate(Age = 2022 - year_built) %>%
  dplyr::select(sale_price, starts_with("crime_")) %>%
  filter(sale_price <= 1000000) %>%
  gather(Variable, Value, -sale_price) %>% 
   ggplot(aes(Value, sale_price)) +
     geom_point(size = .5) + 
     geom_smooth(data = . %>% filter(sale_price > 0), method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, nrow = 1, scales = "free") +
     labs(title = "Price as a function of continuous variables") +
     plotTheme()
```
```{r}
# the result above is too similar, So we calculate and compare the R^2 to choose which crime data to use.
# The result shows that crime_nn3 is the best one.

Crime1Reg <- lm(sale_price ~ crime_nn1, data = philly_sub_200k)
summary(Crime1Reg)

Crime2Reg <- lm(sale_price ~ crime_nn2, data = philly_sub_200k)
summary(Crime2Reg)

Crime3Reg <- lm(sale_price ~ crime_nn3, data = philly_sub_200k)
summary(Crime3Reg)

Crime4Reg <- lm(sale_price ~ crime_nn4, data = philly_sub_200k)
summary(Crime4Reg)

Crime5Reg <- lm(sale_price ~ crime_nn5, data = philly_sub_200k)
summary(Crime5Reg)

```

## Correlation matrix

A correlation matrix gives us the pairwise correlation of each set of features in our data. It is usually advisable to include the target/outcome variable in this so we can understand which features are related to it.

Some things to notice in this code; we use `select_if()` to select only the features that are numeric. This is really handy when you don't want to type or hard-code the names of your features; `ggcorrplot()` is a function from the `ggcorrplot` package.

**Let's take a few minutes to interpret this**

```{r correlation_matrix}
numericVars <- 
  select_if(st_drop_geometry(to_train), is.numeric) %>% na.omit()

ggcorrplot(
  round(cor(numericVars), 1), 
  p.mat = cor_pmat(numericVars),
  colors = c("#25CB10", "white", "#FA7800"),
  type="lower",
  insig = "blank") +  
    labs(title = "Correlation across numeric variables") 


#有无穷或遗漏值？
# yet another way to plot the correlation plot using the corrr library
#numericVars %>% 
#  correlate() %>% 
#  autoplot() +
#  geom_text(aes(label = round(r,digits=2)),size = 2)

```

# Univarite correlation -> multi-variate OLS regression

### Pearson's r - Correlation Coefficient

Pearson's r Learning links:
*   [Pearson Correlation Coefficient (r) | Guide & Examples](https://www.scribbr.com/statistics/pearson-correlation-coefficient/)
*   [Correlation Test Between Two Variables in R](http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r)

Note: the use of the `ggscatter()` function from the `ggpubr` package to plot the *Pearson's rho* or *Pearson's r* statistic; the Correlation Coefficient. This number can also be squared and represented as `r2`. However, this differs from the `R^2` or `R2` or "R-squared" of a linear model fit, known as the Coefficient of Determination. This is explained a bit more below.

```{r uni_variate_Regression}
philly_sub_200k <- st_drop_geometry(to_train) %>% 
filter(sale_price <= 2000000, total_livable_area < 10000, total_livable_area > 0) 

cor.test(philly_sub_200k$total_livable_area,
         philly_sub_200k$sale_price, 
         method = "pearson")

ggscatter(philly_sub_200k,
          x = "total_livable_area",
          y = "sale_price",
          add = "reg.line") +
  stat_cor(label.y = 2500000) 

```


The Pearson's rho - Correlation Coefficient and the R2 Coefficient of Determination are **very** frequently confused! It is a really common mistake, so take a moment to understand what they are and how they differ. [This blog](https://towardsdatascience.com/r%C2%B2-or-r%C2%B2-when-to-use-what-4968eee68ed3) is a good explanation. In summary:

*   The `r` is a measure the degree of relationship between two variables say x and y. It can go between -1 and 1.  1 indicates that the two variables are moving in unison.

*   However, `R2` shows percentage variation in y which is explained by all the x variables together. Higher the better. It is always between 0 and 1. It can never be negative – since it is a squared value.

## Univarite Regression

### R2 - Coefficient of Determination

Discussed above, the `R^2` or "R-squared" is a common way to validate the predictions of a linear model. Below we run a linear model on our data with the `lm()` function and get the output in our R terminal. At first this is an intimidating amount of information! Here is a [great resource](https://towardsdatascience.com/understanding-linear-regression-output-in-r-7a9cbda948b3) to understand how that output is organized and what it means.  

What we are focusing on here is that `R-squared`,  `Adjusted R-squared` and the `Coefficients`.

What's the `R2` good for as a diagnostic of model quality?

Can somebody interpret the coefficient?

Note: Here we use `ggscatter` with the `..rr.label` argument to show the `R2` Coefficient of Determination.

```{r simple_reg}
livingReg <- lm(sale_price ~ total_livable_area, data = philly_sub_200k)

summary(livingReg)

ggscatter(philly_sub_200k,
          x = "total_livable_area",
          y = "sale_price",
          add = "reg.line") +
  stat_cor(aes(label = paste(..rr.label.., ..p.label.., sep = "~`,`~")), label.y = 2500000) +
  stat_regline_equation(label.y = 2250000) 
```


## Prediction example

Make a prediction using the coefficient, intercept etc.,

```{r calculate prediction}
coefficients(livingReg)

new_total_livable_area = 4000

# "by hand"
-41433.9841  + 88.34939 * new_total_livable_area

# predict() function
predict(livingReg, newdata = data.frame(total_livable_area = 4000))
```


## Multivariate Regression

Let's take a look at this regression - how are we creating it?

What's up with these categorical variables?

Better R-squared - does that mean it's a better model?

off_street_open, R_TOTAL_RM, NUM_FLOORS,
                                               R_BDRMS, R_FULL_BTH, R_HALF_BTH, 
                                               R_KITCH, R_AC, R_FPLACE

```{r mutlivariate_regression}
reg1 <- lm(sale_price ~ ., data = philly_sub_200k %>% 
                                 dplyr::select(sale_price, total_livable_area, crime_nn3 ))

summary(reg1)

```

## Marginal Response Plots

Let's try some of these out. They help you learn more about the relationships in the model.

What does a long line on either side of the blue circle suggest?

What does the location of the blue circle relative to the center line at zero suggest?

```{r effect_plots}
## Plot of marginal response
effect_plot(reg1, pred = total_livable_area, interval = TRUE, plot.points = TRUE)

## Plot coefficients
plot_summs(reg1, scale = TRUE)

## plot multiple model coeffs
plot_summs(reg1, livingReg)


```

Challenges:

-What is the Coefficient of total_livable_area when Average Distance to 2-nearest crimes are considered?

-Build a regression with total_livable_area and crime_nn2? Report the regression coefficient for total_livable_area. Is it different than it was before? Why?

- Try to engineer a 'fixed effect' out of the other variables in an attempt to parameterize a variable that suggests a big or fancy house or levels of fanciness. How does this affect your model?

```{r}


```




## Split Data into Train/Test Set
```{r}

inTrain <- createDataPartition(
              y = paste(to_train$building_code_description, to_train$quality_grade), 
              p = .60, list = FALSE)
philly.training <- to_train[inTrain,] 
philly.test <- to_train[-inTrain,]  
 
reg.training <- 
  lm(sale_price ~ ., data = as.data.frame(philly.training) %>% 
                             dplyr::select(sale_price, total_livable_area, crimes.Buffer))

philly.test <-
  philly.test %>%
  mutate(Regression = "Baseline Regression",
         sale_price.Predict = predict(reg.training, philly.test),
         sale_price.Error = sale_price.Predict - sale_price,
         sale_price.AbsError = abs(sale_price.Predict - sale_price),
         sale_price.APE = (abs(sale_price.Predict - sale_price)) / sale_price.Predict)%>%
  filter(sale_price < 5000000) 

```

```{r}
# Remove invalid predictions (Maybe need to do this before running the test)
philly.test <-  philly.test[!with(philly.test,is.na(sale_price.Predict)),]
```
## Spatial Lags

What is the relationship between errors? Are they clustered? Is the error of a single observation correlated with the error of nearby observations?

We create a list of "neigbhors" using a "spatial weights matrix".

```{r}
coords <- st_coordinates(philly.test) 

neighborList <- knn2nb(knearneigh(coords, 5))

spatialWeights <- nb2listw(neighborList, style="W")

philly.test$lagPrice <- lag.listw(spatialWeights, philly.test$sale_price)

```


```{r}
coords.test <-  st_coordinates(philly.test) 

neighborList.test <- knn2nb(knearneigh(coords.test, 5))

spatialWeights.test <- nb2listw(neighborList.test, style="W")


ggplot() +
  geom_point(data = philly.test, aes(x = lagPrice, y = sale_price), size = 2) +
  geom_smooth(data = philly.test, aes(x = lagPrice, y = sale_price), method = "lm", se = F, colour = "#FA7800") +
  labs(title = "Price as a function of the spatial lag of price",
       x = "Spatial Lag of Price (Mean price of 5 nearest neighbors)",
       y = "Sale Price")

philly.test %>% 
  mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)) %>%
  ggplot()+
  geom_point(aes(x =lagPriceError, y =sale_price.Error)) +
  geom_smooth(aes(x = lagPriceError, y = sale_price.Error),method = "lm", se=F, colour = "#FA7800") +
  labs(title = "Error as a function of the spatial lag of price",
       x = "Spatial Lag of Error (Mean error of 5 nearest neighbors)", y = "Sale Price") 

```

## Do Errors Cluster? Using Moran's I

So - is your Moran's I statistic indicating dispersion (-1), randomness (0) or clustering (1)?


```{r}
moranTest <- moran.mc(philly.test$sale_price.Error, 
                      spatialWeights.test, nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  plotTheme()

```

## Predictions by neighborhood

```{r}
philly.test %>%
as.data.frame() %>%
# Tract is not a good factor here
  group_by(census_tract) %>%
    summarize(meanPrediction = mean(sale_price.Predict),
              meanPrice = mean(sale_price)) %>%
      kable() %>% 
  kable_styling()

```

## Regression with neighborhood effects

Let's try to run the regression again, but this time with a neighborhood fixed effect

```{r}
reg.nhood <- lm(sale_price ~ ., data = as.data.frame(philly.training) %>% 
                                 dplyr::select(census_tract, sale_price,
                                              total_livable_area, crimes.Buffer))

philly.test.nhood <-
  philly.test %>%
  mutate(Regression = "Neighborhood Effects",
         sale_price.Predict = predict(reg.nhood, philly.test),
         sale_price.Error = sale_price.Predict- sale_price,
         sale_price.AbsError = abs(sale_price.Predict- sale_price),
         sale_price.APE = (abs(sale_price.Predict- sale_price)) / sale_price)%>%
  filter(sale_price < 5000000)

```

How do these models compare? We can bind our error info together and then examine!

```{r}
bothRegressions <- 
  rbind(
    dplyr::select(philly.test, starts_with("sale_price"), Regression, census_tract) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)),
    dplyr::select(philly.test.nhood, starts_with("sale_price"), Regression, census_tract) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)))  

```

Why do these values differ from those in the book?

```{r}
st_drop_geometry(bothRegressions) %>%
  gather(Variable, Value, -Regression, -census_tract) %>%
  filter(Variable == "sale_price.AbsError" | Variable == "sale_price.APE") %>%
  group_by(Regression, Variable) %>%
    summarize(meanValue = mean(Value, na.rm = T)) %>%
    spread(Variable, meanValue) %>%
    kable()
```

## Further examination of errors

Predicted versus observed plots - what does it mean if the line is above or below y=x?

```{r}
bothRegressions %>%
  dplyr::select(sale_price.Predict, sale_price, Regression) %>%
    ggplot(aes(sale_price, sale_price.Predict)) +
  geom_point() +
  stat_smooth(aes(sale_price, sale_price.Predict), 
             method = "lm", se = FALSE, size = 1, colour="#FA7800") + 
  stat_smooth(aes(sale_price.Predict, sale_price), 
              method = "lm", se = FALSE, size = 1, colour="#25CB10") +
  facet_wrap(~Regression) +
  labs(title="Predicted sale price as a function of observed price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
  plotTheme()

```

We can also examine the spatial pattern of errors.

```{r}
st_drop_geometry(bothRegressions) %>%
  group_by(Regression, sale_price) %>%
  summarize(mean.MAPE = mean(sale_price.APE, na.rm = TRUE), .groups = "drop") %>%
  ungroup() %>%
  left_join(philly.test, by = c(#有问题"Name" = "neighborhood")) %>%
  st_sf() %>%
  ggplot() + 
  geom_sf(aes(fill = mean.MAPE)) +
  geom_sf(data = bothRegressions, colour = "black", size = 0.5) +
  facet_wrap(~Regression) +
  scale_fill_gradient(low = palette5[1], high = palette5[5],
                      name = "MAPE") +
  labs(title = "Mean test set MAPE by neighborhood") +
  mapTheme()

```

## Race and income context of predictions

What is the race and income context of Boston census tracts, and how does this relate to our model performance?

```{r}
tracts20 <- 
  get_acs(geography = "tract", variables = c("B25026_001E","B02001_002E","B19013_001E"), 
          year=2020, state=42, county=101, geometry=TRUE, output="wide") %>%
  st_transform('ESRI:102729') %>% 
  rename(TotalPop = B25026_001E,
         NumberWhites =B02001_002E,
         Median_Income = B19013_001E) %>%
  mutate(percentWhite = NumberWhites / TotalPop,
         raceContext = ifelse(percentWhite > .5, "Majority White", "Majority Non-White"),
         incomeContext = ifelse(Median_Income > 32322, "High Income", "Low Income"))
  
grid.arrange(ncol = 2,
  ggplot() + geom_sf(data = na.omit(tracts20), aes(fill = raceContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Race Context") +
    labs(title = "Race Context") +
    mapTheme() + theme(legend.position="bottom"), 
  ggplot() + geom_sf(data = na.omit(tracts20), aes(fill = incomeContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Income Context") +
    labs(title = "Income Context") +
    mapTheme() + theme(legend.position="bottom"))

```


```{r}
st_join(bothRegressions, tracts17) %>% 
  filter(!is.na(incomeContext)) %>%
  group_by(Regression, incomeContext) %>%
  summarize(mean.MAPE = scales::percent(mean(SalePrice.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(incomeContext, mean.MAPE) %>%
  kable(caption = "Test set MAPE by neighborhood income context")
```
