---
title: "MUSA 508, Lab 4 - Spatial Machine Learning Pt. 1"
author: "Harris, Fichman, and Steif - 2022/23"
output: html_document
---

# 1 Introduction

# 2 Data Wrangling

## 2.1 Set up

```{r setup, include=FALSE}

# You can set some global options for knitting chunks

knitr::opts_chunk$set(echo = TRUE)

# Load some libraries

library(tidyverse)
library(tidycensus)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(broom)
library(tufte)
library(rmarkdown)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots

# functions and data directory
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

palette5 <- c("#25CB10", "#5AB60C", "#8FA108",   "#C48C04", "#FA7800")
```

## 2.2 Data Collection

```{r load_key, warning = FALSE, eval = FALSE}
census_api_key("5c7b1ebb206012789759942ddf1acbb882f937ad", overwrite = TRUE)
```

```{r acs_vars}
acs_vars <- c("B01001_001E", # ACS total Pop estimate
              "B25002_001E", # Estimate of total housing units
              "B25002_003E", # Number of vacant housing units
              "B19013_001E", # Median HH Income ($)
              "B02001_002E", # People describing themselves as "white alone"
              "B06009_006E") # Total graduate or professional degree
```

```{r get_acs_2020, cache = TRUE, message = FALSE, warning = FALSE}
acsTractsPHL.2020 <- get_acs(geography = "tract",
                             year = 2020, 
                             variables = acs_vars, 
                             geometry = TRUE, 
                             state = "PA", 
                             county = "Philadelphia", 
                             output = "wide") %>%
                            st_transform('ESRI:102729')
```

```{r read_data}

nhoods <- 
  st_read("D:/Upenn/23fall/MUSA5080 Public Policy Analysis/midterm 4-5/studentData.geojson") %>%
  st_transform('ESRI:102729')

to_predict <-
  nhoods %>%
  dplyr::filter(toPredict == 'CHALLENGE') #Select data for later prediction

to_train <-
  nhoods %>%
  dplyr::filter(toPredict == 'MODELLING') #Select data for training Model

#??
#Philadelphia <- 
 # read.csv(file.path(root.dir,"/Chapter3_4/phillyHousePriceData_clean.csv"))


# Load crime data
philadelphiCrimes <- read.csv('https://raw.githubusercontent.com/ObjQIAN/MUSA-508-Midterm/main/data/Philadelphia_crime.csv') 

# Create sf and select Weapon Violations
philadelphiCrimes.sf <-
  philadelphiCrimes %>%
  filter(text_general_code == "Weapon Violations",
  lat > -1) %>%
  dplyr::select(lat, lng) %>%
  na.omit() %>%
  st_as_sf(coords = c( "lng","lat"), crs = "EPSG:4326") %>%
  st_transform('ESRI:102729') %>%
  distinct()

# Load 311 data
philadelphia311.sf <- read.csv('https://phl.carto.com/api/v2/sql?filename=public_cases_fc&format=csv&skipfields=cartodb_id,the_geom,the_geom_webmercator&q=SELECT%20*%20FROM%20public_cases_fc%20WHERE%20requested_datetime%20%3E=%20%272022-01-01%27%20AND%20requested_datetime%20%3C%20%272023-01-01%27') %>%
  dplyr::select(lat, lon) %>%
  na.omit() %>%
  st_as_sf(coords = c("lon", "lat"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102729')

```

```{r price_map}
# ggplot, reorder

# Mapping data
ggplot() +
  geom_sf(data = acsTractsPHL.2020, fill = "grey40") +
  geom_sf(data = to_train, aes(colour = q5(sale_price)), 
          show.legend = "point", size = .35) +
  scale_colour_manual(values = palette5,
                   labels=qBr(to_train,"sale_price"),
                   name="Quintile\nBreaks") +
  labs(title="House Sale Price, Philadelphia") +
  mapTheme()
```

## 2.3 Create Nearest Spatial Features

### 2.3.1 Crime Data Set

```{r Features}

# Counts of crime per buffer of house sale
to_train$crimes.Buffer <- to_train %>% 
    st_buffer(660) %>% 
    aggregate(mutate(philadelphiCrimes.sf, counter = 1),., sum) %>%
    pull(counter)


## Nearest Neighbor Feature

to_train <-
  to_train %>% 
    mutate(
      crime_nn1 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphiCrimes.sf), k = 1),
      
      crime_nn2 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphiCrimes.sf), k = 2), 
      
      crime_nn3 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphiCrimes.sf), k = 3), 
      
      crime_nn4 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphiCrimes.sf), k = 4), 
      
      crime_nn5 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphiCrimes.sf), k = 5)) 
```

```{r assault density}
## Plot assault density
ggplot() + geom_sf(data = acsTractsPHL.2020, fill = "grey40") +
  stat_density2d(data = data.frame(st_coordinates(philadelphiCrimes.sf)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "#25CB10", high = "#FA7800", name = "Density") +
  scale_alpha(range = c(0.00, 0.35), guide = "none") +
  labs(title = "Density of WA, PHL") +
  mapTheme()
```

### 2.3.2 311 Data Set

```{r Features}

# Counts of 311 per buffer of house sale
to_train$philly311.Buffer <- to_train %>% 
    st_buffer(660) %>% 
    aggregate(mutate(philadelphia311.sf, counter = 1),., sum) %>%
    pull(counter)


## Nearest Neighbor Feature

to_train <-
  to_train %>% 
    mutate(
      p311_nn1 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphia311.sf), k = 1),
      
      p311_nn2 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphia311.sf), k = 2), 
      
      p311_nn3 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphia311.sf), k = 3), 
      
      p311_nn4 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphia311.sf), k = 4), 
      
      p311_nn5 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphia311.sf), k = 5)) 
```

```{r 311 density}
## Plot 311 density-????
ggplot() + geom_sf(data = acsTractsPHL.2020, fill = "grey40") +
  stat_density2d(data = data.frame(st_coordinates(philadelphia311.sf)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "#25CB10", high = "#FA7800", name = "Density") +
  scale_alpha(range = c(0.1, 0.35), guide = "none") +
  labs(title = "Density of 311, PHL") +
  mapTheme()
```

# 3 Analyzing Associations

Run these code blocks...
Notice the use of `st_drop_geometry()`, this is the correct way to go from a `sf` spatial dataframe to a regular dataframe with no spatial component.

Can somebody walk me through what they do?

Can you give me a one-sentence description of what the takeaway is?

## 3.1 Sale Price as a Function of Numeric Features

```{r}

## Crime cor
to_train %>%
  st_drop_geometry() %>%
  mutate(Age = 2022 - year_built) %>%
  dplyr::select(sale_price, starts_with("crime_")) %>%
  filter(sale_price <= 1000000) %>%
  gather(Variable, Value, -sale_price) %>% 
   ggplot(aes(Value, sale_price)) +
     geom_point(size = .5) + 
     geom_smooth(data = . %>% filter(sale_price > 0), method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, nrow = 1, scales = "free") +
     labs(title = "Price as a function of continuous variables") +
     plotTheme()

```
The result shown in the plot above is too similar, so we calculate and compare the R^2 to choose which crime data to use.
The result shows that crime_nn3 is the best one (Adjusted R-squared:  0.1118).
```{r}
# the result above is too similar, So we calculate and compare the R^2 to choose which crime data to use.
# The result shows that crime_nn3 is the best one.
to_train <- to_train %>%
  mutate(Age = 2022 - year_built) 

philly_sub_200k <- st_drop_geometry(to_train) %>% 
filter(sale_price <= 2000000, total_livable_area < 10000, total_livable_area > 0) 

Crime1Reg <- lm(sale_price ~ crime_nn1, data = philly_sub_200k)
summary(Crime1Reg)

Crime2Reg <- lm(sale_price ~ crime_nn2, data = philly_sub_200k)
summary(Crime2Reg)

Crime3Reg <- lm(sale_price ~ crime_nn3, data = philly_sub_200k)
summary(Crime3Reg)

Crime4Reg <- lm(sale_price ~ crime_nn4, data = philly_sub_200k)
summary(Crime4Reg)

Crime5Reg <- lm(sale_price ~ crime_nn5, data = philly_sub_200k)
summary(Crime5Reg)

```

We use the scatter plot to initially determine whether the following Numeric Features are suitable as our variables. As can be seen from the chart below, some of the features that may be useful are: `Age`, `crime_nn3`, `depth`, `frontage`, `philly311.Buffer`, `total_livable_area`.
(若结果拟合度不足可以考虑继续添加)

```{r Correlation}

## Home Features cor

st_drop_geometry(to_train) %>% 
  dplyr::select(sale_price, total_livable_area, Age, crime_nn3, 
                depth, frontage, off_street_open, philly311.Buffer) %>%
  filter(sale_price <= 1000000, Age < 500, total_livable_area <10000, depth < 600, frontage < 500) %>%
  gather(Variable, Value, -sale_price) %>% 
   ggplot(aes(Value, sale_price)) +
     geom_point(size = .5) + 
    geom_smooth(data = . %>% filter(sale_price >0), method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, ncol = 3, scales = "free") +
     labs(title = "Price as a function of continuous variables") +
     plotTheme()

```

## 3.2 Sale Price as a Function of Categorical Features

We use the histograms to initially determine whether the following categorical features are suitable as our variables. As can be seen from the chart below, some of the features that may be useful are: `exterior_condition`, `fireplaces`, `interior_condition`, `number_of_bathrooms`, `number_of_bedrooms`, `number_stories`.
(若结果拟合度不足可以考虑继续添加)

```{r}

#这里的代码我不知道要怎么合并一下好

to_train %>%
  dplyr::select(sale_price, exterior_condition) %>%
  mutate(exterior_condition = as.factor(exterior_condition)) %>%
  filter(sale_price <= 1000000) %>%
  group_by(exterior_condition) %>%
  summarize(avg_sale_price = mean(sale_price)) %>%
  ggplot(aes(x = exterior_condition, y = avg_sale_price)) +
    geom_bar(stat = "identity") +
    labs(
      title = "Average Sale Price by Exterior Condition",
      y = "Average Sale Price"
    ) +
    plotTheme() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

to_train %>%
  dplyr::select(sale_price, fireplaces) %>%
  mutate(fireplaces = as.factor(fireplaces)) %>%
  filter(sale_price <= 1000000) %>%
  filter(!is.na(fireplaces)) %>%
  group_by(fireplaces) %>%
  summarize(avg_sale_price = mean(sale_price)) %>%
  ggplot(aes(x = fireplaces, y = avg_sale_price)) +
    geom_bar(stat = "identity") +
    labs(
      title = "Average Sale Price by Exterior Condition",
      y = "Average Sale Price"
    ) +
    plotTheme() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

to_train %>%
  dplyr::select(sale_price, interior_condition) %>%
  mutate(interior_condition = as.factor(interior_condition)) %>%
  filter(sale_price <= 1000000) %>%
  filter(!is.na(interior_condition)) %>%
  group_by(interior_condition) %>%
  summarize(avg_sale_price = mean(sale_price)) %>%
  ggplot(aes(x = interior_condition, y = avg_sale_price)) +
    geom_bar(stat = "identity") +
    labs(
      title = "Average Sale Price by Exterior Condition",
      y = "Average Sale Price"
    ) +
    plotTheme() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

to_train %>%
  dplyr::select(sale_price, number_of_bathrooms) %>%
  mutate(number_of_bathrooms = as.factor(number_of_bathrooms)) %>%
  filter(sale_price <= 1000000) %>%
  filter(!is.na(number_of_bathrooms)) %>%
  group_by(number_of_bathrooms) %>%
  summarize(avg_sale_price = mean(sale_price)) %>%
  ggplot(aes(x = number_of_bathrooms, y = avg_sale_price)) +
    geom_bar(stat = "identity") +
    labs(
      title = "Average Sale Price by Exterior Condition",
      y = "Average Sale Price"
    ) +
    plotTheme() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

to_train %>%
  dplyr::select(sale_price, number_of_bedrooms) %>%
  mutate(number_of_bedrooms = as.factor(number_of_bedrooms)) %>%
  filter(sale_price <= 1000000) %>%
  filter(!is.na(number_of_bedrooms)) %>%
  group_by(number_of_bedrooms) %>%
  summarize(avg_sale_price = mean(sale_price)) %>%
  ggplot(aes(x = number_of_bedrooms, y = avg_sale_price)) +
    geom_bar(stat = "identity") +
    labs(
      title = "Average Sale Price by Exterior Condition",
      y = "Average Sale Price"
    ) +
    plotTheme() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

to_train %>%
  dplyr::select(sale_price, number_stories) %>%
  mutate(number_stories = as.factor(number_stories)) %>%
  filter(sale_price <= 1000000) %>%
  filter(!is.na(number_stories)) %>%
  group_by(number_stories) %>%
  summarize(avg_sale_price = mean(sale_price)) %>%
  ggplot(aes(x = number_stories, y = avg_sale_price)) +
    geom_bar(stat = "identity") +
    labs(
      title = "Average Sale Price by Exterior Condition",
      y = "Average Sale Price"
    ) +
    plotTheme() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

## 3.3 Select Variables

In order to prevent the selected features from having too strong correlations between them and affecting the regression results, we use a correlation matrix to provide us with pairwise correlations for each set of features in the data, and use Pearson's r - Correlation Coefficient to Look at the contribution of each variable. 

### 3.3.1  Correlation matrix

The correlation matrix reveals a notable association between `exterior_condition` and `interior_condition`. Upon closer examination, it becomes evident that `interior_condition` offers a more robust explanation for fluctuations in `sale_price` (R-squared for `exterior` = 0.168, R-squared for `interior` = 0.1699). 

Furthermore, `Total_livable_area` demonstrates a substantial correlation with both `frontage` and `depth`. Our choice to include `Total_livable_area` is substantiated by the steepness of the regression line observed in the scatterplot.

To encapsulate, the chosen numeric features encompass `Age`, `crime_nn3`, `philly311.Buffer`, `total_livable_area`, while the selected categorical features comprise `fireplaces`, `interior_condition`, `number_of_bathrooms`, `number_of_bedrooms`, and `number_stories`.

```{r correlation_matrix}
numericVars <- 
  select_if(st_drop_geometry(to_train), is.numeric) %>% na.omit()

ggcorrplot(
  round(cor(numericVars), 1), 
  p.mat = cor_pmat(numericVars),
  colors = c("#25CB10", "white", "#FA7800"),
  type="lower",
  insig = "blank") +  
    labs(title = "Correlation across numeric variables") 


#有无穷或遗漏值？
# yet another way to plot the correlation plot using the corrr library
#numericVars %>% 
#  correlate() %>% 
#  autoplot() +
#  geom_text(aes(label = round(r,digits=2)),size = 2)

```
```{r}

ex_Reg <- lm(sale_price ~ exterior_condition, data = philly_sub_200k)
summary(ex_Reg)

in_Reg <- lm(sale_price ~ interior_condition, data = philly_sub_200k)
summary(in_Reg)

```

### 3.3.2 Univarite correlation with Pearson's r - Correlation Coefficient

```{r uni_variate_Regression}

cor.test(philly_sub_200k$total_livable_area,
         philly_sub_200k$sale_price, 
         method = "pearson")

```

## Univarite Regression

### R2 - Coefficient of Determination

```{r simple_reg}
livingReg <- lm(sale_price ~ total_livable_area, data = philly_sub_200k)

summary(livingReg)

ggscatter(philly_sub_200k,
          x = "total_livable_area",
          y = "sale_price",
          add = "reg.line") +
  stat_cor(aes(label = paste(..rr.label.., ..p.label.., sep = "~`,`~")), label.y = 2500000) +
  stat_regline_equation(label.y = 2250000) 

```


## Prediction example

Make a prediction using the coefficient, intercept etc.,

```{r calculate prediction}
coefficients(livingReg)

new_total_livable_area = 4000

# "by hand"
-41433.9841  + 88.34939 * new_total_livable_area

# predict() function
predict(livingReg, newdata = data.frame(total_livable_area = 4000))

```


# 4 Multivariate OLS Regression


```{r mutlivariate_regression}
reg1 <- lm(sale_price ~ ., data = philly_sub_200k %>% 
                                 dplyr::select(sale_price, Age, total_livable_area, crime_nn3, philly311.Buffer, 
                                               fireplaces, interior_condition, number_of_bathrooms, number_of_bedrooms, number_stories))

summary(reg1)

```

## Marginal Response Plots

Let's try some of these out. They help you learn more about the relationships in the model.

What does a long line on either side of the blue circle suggest?

What does the location of the blue circle relative to the center line at zero suggest?

```{r effect_plots}
## Plot of marginal response
effect_plot(reg1, pred = total_livable_area, interval = TRUE, plot.points = TRUE)

## Plot coefficients
plot_summs(reg1, scale = TRUE)

## plot multiple model coeffs
plot_summs(reg1, livingReg)


```

Challenges:

-What is the Coefficient of total_livable_area when Average Distance to 2-nearest crimes are considered?

-Build a regression with total_livable_area and crime_nn2? Report the regression coefficient for total_livable_area. Is it different than it was before? Why?

- Try to engineer a 'fixed effect' out of the other variables in an attempt to parameterize a variable that suggests a big or fancy house or levels of fanciness. How does this affect your model?

```{r}


```




## Split Data into Train/Test Set
```{r}

inTrain <- createDataPartition(
              y = paste(to_train$building_code_description, to_train$quality_grade), 
              p = .60, list = FALSE)
philly.training <- to_train[inTrain,] 
philly.test <- to_train[-inTrain,]  
 
reg.training <- 
  lm(sale_price ~ ., data = as.data.frame(philly.training) %>% 
                             dplyr::select(sale_price, total_livable_area, crimes.Buffer))

philly.test <-
  philly.test %>%
  mutate(Regression = "Baseline Regression",
         sale_price.Predict = predict(reg.training, philly.test),
         sale_price.Error = sale_price.Predict - sale_price,
         sale_price.AbsError = abs(sale_price.Predict - sale_price),
         sale_price.APE = (abs(sale_price.Predict - sale_price)) / sale_price.Predict)%>%
  filter(sale_price < 5000000) 

```

```{r}
# Remove invalid predictions (Maybe need to do this before running the test)
philly.test <-  philly.test[!with(philly.test,is.na(sale_price.Predict)),]
```
## 5 Spatial Lags

What is the relationship between errors? Are they clustered? Is the error of a single observation correlated with the error of nearby observations?

We create a list of "neigbhors" using a "spatial weights matrix".

```{r}
coords <- st_coordinates(philly.test) 

neighborList <- knn2nb(knearneigh(coords, 5))

spatialWeights <- nb2listw(neighborList, style="W")

philly.test$lagPrice <- lag.listw(spatialWeights, philly.test$sale_price)

```


```{r}
coords.test <-  st_coordinates(philly.test) 

neighborList.test <- knn2nb(knearneigh(coords.test, 5))

spatialWeights.test <- nb2listw(neighborList.test, style="W")

ggplot() +
  geom_point(data = philly.test, aes(x = lagPrice, y = sale_price), size = 2) +
  geom_smooth(data = philly.test, aes(x = lagPrice, y = sale_price), method = "lm", se = F, colour = "#FA7800") +
  labs(title = "Price as a function of the spatial lag of price",
       x = "Spatial Lag of Price (Mean price of 5 nearest neighbors)",
       y = "Sale Price")

philly.test %>% 
  mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)) %>%
  ggplot()+
  geom_point(aes(x =lagPriceError, y =sale_price.Error)) +
  geom_smooth(aes(x = lagPriceError, y = sale_price.Error),method = "lm", se=F, colour = "#FA7800") +
  labs(title = "Error as a function of the spatial lag of price",
       x = "Spatial Lag of Error (Mean error of 5 nearest neighbors)", y = "Sale Price") 

```

## Do Errors Cluster? Using Moran's I

So - is your Moran's I statistic indicating dispersion (-1), randomness (0) or clustering (1)?


```{r}
moranTest <- moran.mc(philly.test$sale_price.Error, 
                      spatialWeights.test, nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  plotTheme()

```

## Predictions by neighborhood

```{r}
philly.test %>%
as.data.frame() %>%
# Tract is not a good factor here
  group_by(zip_code) %>%
    summarize(meanPrediction = mean(sale_price.Predict),
              meanPrice = mean(sale_price)) %>%
      kable() %>% 
  kable_styling()

```

## Regression with neighborhood effects

Let's try to run the regression again, but this time with a neighborhood fixed effect

```{r}
reg.nhood <- lm(sale_price ~ ., data = as.data.frame(philly.training) %>% 
                                 dplyr::select(zip_code, sale_price,
                                              total_livable_area, crimes.Buffer))

philly.test.nhood <-
  philly.test %>%
  mutate(Regression = "Neighborhood Effects",
         sale_price.Predict = predict(reg.nhood, philly.test),
         sale_price.Error = sale_price.Predict- sale_price,
         sale_price.AbsError = abs(sale_price.Predict- sale_price),
         sale_price.APE = (abs(sale_price.Predict- sale_price)) / sale_price)%>%
  filter(sale_price < 5000000)

```

How do these models compare? We can bind our error info together and then examine!

```{r}
bothRegressions <- 
  rbind(
    dplyr::select(philly.test, starts_with("sale_price"), Regression, zip_code) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)),
    dplyr::select(philly.test.nhood, starts_with("sale_price"), Regression, zip_code) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)))  

```

Why do these values differ from those in the book?

```{r}
st_drop_geometry(bothRegressions) %>%
  gather(Variable, Value, -Regression, -zip_code) %>%
  filter(Variable == "sale_price.AbsError" | Variable == "sale_price.APE") %>%
  group_by(Regression, Variable) %>%
    summarize(meanValue = mean(Value, na.rm = T)) %>%
    spread(Variable, meanValue) %>%
    kable()
```

## Further examination of errors

Predicted versus observed plots - what does it mean if the line is above or below y=x?

```{r}
bothRegressions %>%
  dplyr::select(sale_price.Predict, sale_price, Regression) %>%
    ggplot(aes(sale_price, sale_price.Predict)) +
  geom_point() +
  stat_smooth(aes(sale_price, sale_price.Predict), 
             method = "lm", se = FALSE, size = 1, colour="#FA7800") + 
  stat_smooth(aes(sale_price.Predict, sale_price), 
              method = "lm", se = FALSE, size = 1, colour="#25CB10") +
  facet_wrap(~Regression) +
  labs(title="Predicted sale price as a function of observed price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
  plotTheme()

```

We can also examine the spatial pattern of errors.

```{r}

st_drop_geometry(bothRegressions) %>%
  group_by(Regression, zip_code) %>%
  summarize(mean.MAPE = mean(sale_price.APE, na.rm = T)) %>%
  ungroup() %>% 
  st_join(acsTractsPHL.2020) %>%
    st_sf() %>%
    ggplot() + 
      geom_sf(aes(fill = mean.MAPE)) +
      geom_sf(data = bothRegressions, colour = "black", size = .1) +
      facet_wrap(~Regression) +
      scale_fill_gradient(low = palette5[1], high = palette5[5],
                          name = "MAPE") +
      labs(title = "Mean test set MAPE by neighborhood") +
      mapTheme()

```

## Race and income context of predictions

What is the race and income context of Boston census tracts, and how does this relate to our model performance?

```{r}
tracts20 <- 
  get_acs(geography = "tract", variables = c("B25026_001E","B02001_002E","B19013_001E"), 
          year=2020, state=42, county=101, geometry=TRUE, output="wide") %>%
  st_transform('ESRI:102729') %>% 
  rename(TotalPop = B25026_001E,
         NumberWhites =B02001_002E,
         Median_Income = B19013_001E) %>%
  mutate(percentWhite = NumberWhites / TotalPop,
         raceContext = ifelse(percentWhite > .5, "Majority White", "Majority Non-White"),
         incomeContext = ifelse(Median_Income > 32322, "High Income", "Low Income"))
  
grid.arrange(ncol = 2,
  ggplot() + geom_sf(data = na.omit(tracts20), aes(fill = raceContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Race Context") +
    labs(title = "Race Context") +
    mapTheme() + theme(legend.position="bottom"), 
  ggplot() + geom_sf(data = na.omit(tracts20), aes(fill = incomeContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Income Context") +
    labs(title = "Income Context") +
    mapTheme() + 
    theme(legend.position="bottom"))

```


```{r}

st_join(bothRegressions, tracts20) %>% 
  group_by(Regression, raceContext) %>%
  summarize(mean.MAPE = scales::percent(mean(sale_price.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(raceContext, mean.MAPE) %>%
  kable(caption = "Test set MAPE by neighborhood racial context")

```
111
