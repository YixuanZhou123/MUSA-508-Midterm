---
title: "MUSA 508, Lab 4 - Spatial Machine Learning Pt. 1"
author: "Harris, Fichman, and Steif - 2022/23"
output: html_document
---

```{r setup, include=FALSE}

# You can set some global options for knitting chunks

knitr::opts_chunk$set(echo = TRUE)

# Load some libraries

library(tidyverse)
library(tidycensus)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(broom)
library(tufte)
library(rmarkdown)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots

# functions and data directory
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

palette5 <- c("#25CB10", "#5AB60C", "#8FA108",   "#C48C04", "#FA7800")
```

# MUSA 508, Lab 4 - Spatial Machine Learning Pt. 1

The learning objectives of this lab are:

*   Review Topics: loading data, {dplyr}, mapping and plotting with {ggplot2}
*   New: Understanding how we created spatial indicators with K-Nearest Neighbor and the `nn_function()` function
*   Simple correlation and the Pearson's r - Correlation Coefficient
*   Linear Regression model goodness-of-fit and R2 - Coefficient of Determination

The in-class exercise at the end encourages students to modify the existing code to create a different regression and interpret it. Finally, there is a prompt to create a new feature and add it to the regression.

## Data Wrangling

See if you can change the chunk options to get rid of the text outputs

```{r load_key, warning = FALSE, eval = FALSE}
census_api_key("5c7b1ebb206012789759942ddf1acbb882f937ad", overwrite = TRUE)
```

```{r acs_vars}
acs_vars <- c("B01001_001E", # ACS total Pop estimate
              "B25002_001E", # Estimate of total housing units
              "B25002_003E", # Number of vacant housing units
              "B19013_001E", # Median HH Income ($)
              "B02001_002E", # People describing themselves as "white alone"
              "B06009_006E") # Total graduate or professional degree
```

```{r get_acs_2020, cache = TRUE, message = FALSE, warning = FALSE}
acsTractsPHL.2020 <- get_acs(geography = "tract",
                             year = 2020, 
                             variables = acs_vars, 
                             geometry = TRUE, 
                             state = "PA", 
                             county = "Philadelphia", 
                             output = "wide") %>%
                            st_transform('ESRI:102729')
```



```{r read_data}

nhoods <- 
  st_read("E:/Downloads/studentData.geojson") %>%
  st_transform('ESRI:102729')

to_predict <-
  nhoods %>%
  dplyr::filter(toPredict == 'CHALLENGE') #Select data for later prediction

#??
#Philadelphia <- 
 # read.csv(file.path(root.dir,"/Chapter3_4/phillyHousePriceData_clean.csv"))


# Load crime data
philadelphiCrimes <- read.csv('https://raw.githubusercontent.com/ObjQIAN/MUSA-508-Midterm/main/data/Philadelphia_crime.csv') 

# Create sf and select Weapon Violations
philadelphiCrimes.sf <-
  philadelphiCrimes %>%
  filter(text_general_code == "Weapon Violations",
  lat > -1) %>%
  dplyr::select(lat, lng) %>%
  na.omit() %>%
  st_as_sf(coords = c( "lng","lat"), crs = "EPSG:4326") %>%
  st_transform('ESRI:102729') %>%
  distinct()




# Load 311 data
philadelphia311 <- read.csv('https://phl.carto.com/api/v2/sql?filename=public_cases_fc&format=csv&skipfields=cartodb_id,the_geom,the_geom_webmercator&q=SELECT%20*%20FROM%20public_cases_fc%20WHERE%20requested_datetime%20%3E=%20%272022-01-01%27%20AND%20requested_datetime%20%3C%20%272023-01-01%27') %>%
  dplyr::select(lat, lon) %>%
  na.omit() %>%
  st_as_sf(coords = c("lon", "lat"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102729')



```



```{r price_map}
# ggplot, reorder

# Mapping data
ggplot() +
  geom_sf(data = acsTractsPHL.2020, fill = "grey40") +
  geom_sf(data = nhoods, aes(colour = q5(sale_price)), 
          show.legend = "point", size = .35) +
  scale_colour_manual(values = palette5,
                   labels=qBr(nhoods,"sale_price"),
                   name="Quintile\nBreaks") +
  labs(title="House Sale Price, Philadelphia") +
  mapTheme()
```


### Create Nearest Spatial Features
This is where we do two important things:

*   aggregate values within a buffer, and 
*   create  `nearest neighbor` features. 

These are primary tools we use to add the local spatial signal to each of the points/rows/observations we are modeling. 

#### Buffer aggregate

The first code in this block buffers each point in `nhoods` by `660` ft and then uses `aggregate` to count the number of `phillyCrimes.sf` points within that buffer. There is a nested call to `mutate` to assign a value of `1` for each `phillyCrimes.sf` as under a column called `counter`. This allows each crime to have the same weight in when the `sun` function is called, but other weighting schemes could be used. Finally, the code `pull` pulls the aggregate values so they can be assigned as the `crimes.Buffer` column to `nhoods`. This is a little different from how you had assigned new columns before (usually using `mutate`), but valid. Your new feature `crimes.Buffer` is a count of all the crimes within 660ft of each reported crime. Why would it be god to know this when building a model?  


#### k nearest neighbor (knn)

The second block of code is using knn for averaging or summing values of a set number of (referred to as `k`) the nearest observations to each observation; it's *nearest neighbors*. With this, the model can understand the magnitude of values that are *near* each point. This adds a spatial signal to the model. 

<!-- The function `nn_function()` takes two pairs of **coordinates** form two `sf` **point** objects. You will see the use of `st_c()` which is just a shorthand way to get the coordinates with the `st_coordinates()` function. You can see where `st_c()` is created. Using `st_c()` within `mutate` converts the points to longitude/latitude coordinate pairs for the `nn_function()` to work with. If you put *polygon* features in there, it will error. Instead you can use `st_c(st_centroid(YourPolygonSfObject))` to get nearest neighbors to a polygon centroid. -->

The number `k` in the `nn_function()` function is the number of neighbors to to values from that are then averaged. Different types of crime (or anything else you measure) will require different values of `k`. *You will have to think about this!*. What could be the importance of `k` when you are making knn features for a violent crime versus and nuisance crime?

```{r Features}

# Counts of crime per buffer of house sale
nhoods$crimes.Buffer <- nhoods %>% 
    st_buffer(660) %>% 
    aggregate(mutate(philadelphiCrimes.sf, counter = 1),., sum) %>%
    pull(counter)


## Nearest Neighbor Feature

nhoods <-
  nhoods %>% 
    mutate(
      crime_nn1 = nn_function(st_coordinates(nhoods), 
                              st_coordinates(philadelphiCrimes.sf), k = 1),
      
      crime_nn2 = nn_function(st_coordinates(nhoods), 
                              st_coordinates(philadelphiCrimes.sf), k = 2), 
      
      crime_nn3 = nn_function(st_coordinates(nhoods), 
                              st_coordinates(philadelphiCrimes.sf), k = 3), 
      
      crime_nn4 = nn_function(st_coordinates(nhoods), 
                              st_coordinates(philadelphiCrimes.sf), k = 4), 
      
      crime_nn5 = nn_function(st_coordinates(nhoods), 
                              st_coordinates(philadelphiCrimes.sf), k = 5)) 
```

```{r assault density}
## Plot assault density
ggplot() + geom_sf(data = acsTractsPHL.2020, fill = "grey40") +
  stat_density2d(data = data.frame(st_coordinates(philadelphiCrimes.sf)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "#25CB10", high = "#FA7800", name = "Density") +
  scale_alpha(range = c(0.00, 0.35), guide = "none") +
  labs(title = "Density of WA, PHL") +
  mapTheme()
```

## Analyzing associations

Run these code blocks...
Notice the use of `st_drop_geometry()`, this is the correct way to go from a `sf` spatial dataframe to a regular dataframe with no spatial component.

Can somebody walk me through what they do?

Can you give me a one-sentence description of what the takeaway is?


```{r Correlation}

## Home Features cor
st_drop_geometry(nhoods) %>% 
  mutate(Age = 2022 - year_built) %>%
  dplyr::select(sale_price, total_livable_area, Age, crimes.Buffer) %>%
  filter(sale_price <= 1000000, Age < 500) %>%
  gather(Variable, Value, -sale_price) %>% 
   ggplot(aes(Value, sale_price)) +
     geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, ncol = 3, scales = "free") +
     labs(title = "Price as a function of continuous variables") +
     plotTheme()
```

Can we resize this somehow to make it look better?

```{r crime_corr}
## Crime cor
nhoods %>%
  st_drop_geometry() %>%
  mutate(Age = 2022 - year_built) %>%
  dplyr::select(sale_price, starts_with("crime_")) %>%
  filter(sale_price <= 1000000) %>%
  gather(Variable, Value, -sale_price) %>% 
   ggplot(aes(Value, sale_price)) +
     geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, nrow = 1, scales = "free") +
     labs(title = "Price as a function of continuous variables") +
     plotTheme()
```

## Correlation matrix

A correlation matrix gives us the pairwise correlation of each set of features in our data. It is usually advisable to include the target/outcome variable in this so we can understand which features are related to it.

Some things to notice in this code; we use `select_if()` to select only the features that are numeric. This is really handy when you don't want to type or hard-code the names of your features; `ggcorrplot()` is a function from the `ggcorrplot` package.

**Let's take a few minutes to interpret this**

```{r correlation_matrix}
numericVars <- 
  select_if(st_drop_geometry(nhoods), is.numeric) %>% na.omit()

ggcorrplot(
  round(cor(numericVars), 1), 
  p.mat = cor_pmat(numericVars),
  colors = c("#25CB10", "white", "#FA7800"),
  type="lower",
  insig = "blank") +  
    labs(title = "Correlation across numeric variables") 

#有无穷或遗漏值？
# yet another way to plot the correlation plot using the corrr library
#numericVars %>% 
#  correlate() %>% 
#  autoplot() +
#  geom_text(aes(label = round(r,digits=2)),size = 2)

```

# Univarite correlation -> multi-variate OLS regression

### Pearson's r - Correlation Coefficient

Pearson's r Learning links:
*   [Pearson Correlation Coefficient (r) | Guide & Examples](https://www.scribbr.com/statistics/pearson-correlation-coefficient/)
*   [Correlation Test Between Two Variables in R](http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r)

Note: the use of the `ggscatter()` function from the `ggpubr` package to plot the *Pearson's rho* or *Pearson's r* statistic; the Correlation Coefficient. This number can also be squared and represented as `r2`. However, this differs from the `R^2` or `R2` or "R-squared" of a linear model fit, known as the Coefficient of Determination. This is explained a bit more below.

```{r uni_variate_Regression}
philly_sub_200k <- st_drop_geometry(nhoods) %>% 
filter(sale_price <= 2000000) 

cor.test(philly_sub_200k$total_livable_area,
         philly_sub_200k$sale_price, 
         method = "pearson")

ggscatter(philly_sub_200k,
          x = "total_livable_area",
          y = "sale_price",
          add = "reg.line") +
  stat_cor(label.y = 2500000) 

```


The Pearson's rho - Correlation Coefficient and the R2 Coefficient of Determination are **very** frequently confused! It is a really common mistake, so take a moment to understand what they are and how they differ. [This blog](https://towardsdatascience.com/r%C2%B2-or-r%C2%B2-when-to-use-what-4968eee68ed3) is a good explanation. In summary:

*   The `r` is a measure the degree of relationship between two variables say x and y. It can go between -1 and 1.  1 indicates that the two variables are moving in unison.

*   However, `R2` shows percentage variation in y which is explained by all the x variables together. Higher the better. It is always between 0 and 1. It can never be negative – since it is a squared value.

## Univarite Regression

### R2 - Coefficient of Determination

Discussed above, the `R^2` or "R-squared" is a common way to validate the predictions of a linear model. Below we run a linear model on our data with the `lm()` function and get the output in our R terminal. At first this is an intimidating amount of information! Here is a [great resource](https://towardsdatascience.com/understanding-linear-regression-output-in-r-7a9cbda948b3) to understand how that output is organized and what it means.  

What we are focusing on here is that `R-squared`,  `Adjusted R-squared` and the `Coefficients`.

What's the `R2` good for as a diagnostic of model quality?

Can somebody interpret the coefficient?

Note: Here we use `ggscatter` with the `..rr.label` argument to show the `R2` Coefficient of Determination.

```{r simple_reg}
livingReg <- lm(sale_price ~ total_livable_area, data = philly_sub_200k)

summary(livingReg)

ggscatter(philly_sub_200k,
          x = "total_livable_area",
          y = "sale_price",
          add = "reg.line") +
  stat_cor(aes(label = paste(..rr.label.., ..p.label.., sep = "~`,`~")), label.y = 2500000) +
  stat_regline_equation(label.y = 2250000) 
```


## Prediction example

Make a prediction using the coefficient, intercept etc.,

```{r calculate prediction}
coefficients(livingReg)

new_total_livable_area = 4000

# "by hand"
378370.01571  + 88.34939 * new_total_livable_area

# predict() function
predict(livingReg, newdata = data.frame(total_livable_area = 4000))
```


## Multivariate Regression

Let's take a look at this regression - how are we creating it?

What's up with these categorical variables?

Better R-squared - does that mean it's a better model?

off_street_open, R_TOTAL_RM, NUM_FLOORS,
                                               R_BDRMS, R_FULL_BTH, R_HALF_BTH, 
                                               R_KITCH, R_AC, R_FPLACE

```{r mutlivariate_regression}
reg1 <- lm(sale_price ~ ., data = philly_sub_200k %>% 
                                 dplyr::select(sale_price, total_livable_area))

summary(reg1)
```

## Marginal Response Plots

Let's try some of these out. They help you learn more about the relationships in the model.

What does a long line on either side of the blue circle suggest?

What does the location of the blue circle relative to the center line at zero suggest?

```{r effect_plots}
## Plot of marginal response
effect_plot(reg1, pred = R_BDRMS, interval = TRUE, plot.points = TRUE)

## Plot coefficients
plot_summs(reg1, scale = TRUE)

## plot multiple model coeffs
plot_summs(reg1, livingReg)


```

Challenges:

-What is the Coefficient of total_livable_area when Average Distance to 2-nearest crimes are considered?

-Build a regression with total_livable_area and crime_nn2? Report the regression coefficient for total_livable_area. Is it different than it was before? Why?

- Try to engineer a 'fixed effect' out of the other variables in an attempt to parameterize a variable that suggests a big or fancy house or levels of fanciness. How does this affect your model?

```{r}


```




## Split Data into Train/Test Set
```{r}

inTrain <- createDataPartition(
              y = paste(nhoods$building_code_description, nhoods$quality_grade), 
              p = .60, list = FALSE)
philly.training <- nhoods[inTrain,] 
philly.test <- nhoods[-inTrain,]  
 
reg.training <- 
  lm(sale_price ~ ., data = as.data.frame(philly.training) %>% 
                             dplyr::select(sale_price,crimes.Buffer,))

philly.test <-
  philly.test %>%
  mutate(Regression = "Baseline Regression",
         sale_price.Predict = predict(reg.training, philly.test),
         sale_price.Error = sale_price.Predict - sale_price,
         sale_price.AbsError = abs(sale_price.Predict - sale_price),
         sale_price.APE = (abs(sale_price.Predict - sale_price)) / sale_price.Predict)%>%
  filter(sale_price < 5000000) 

```

```{r}
# Remove invalid predictions (Maybe need to do this before running the test)
philly.test <-  philly.test[!with(philly.test,is.na(sale_price.Predict)),]
```
## Spatial Lags

What is the relationship between errors? Are they clustered? Is the error of a single observation correlated with the error of nearby observations?

We create a list of "neigbhors" using a "spatial weights matrix".

```{r}
coords <- st_coordinates(nhoods) 

neighborList <- knn2nb(knearneigh(coords, 5))

spatialWeights <- nb2listw(neighborList, style="W")

nhoods$lagPrice <- lag.listw(spatialWeights, nhoods$sale_price)

```


```{r}
coords.test <-  st_coordinates(philly.test) 

neighborList.test <- knn2nb(knearneigh(coords.test, 5))

spatialWeights.test <- nb2listw(neighborList.test, style="W")
 
philly.test %>% 
  mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)) %>%
  ggplot()+
  geom_point(aes(x =lagPriceError, y =sale_price.Error))

```

## Do Errors Cluster? Using Moran's I

So - is your Moran's I statistic indicating dispersion (-1), randomness (0) or clustering (1)?


```{r}
moranTest <- moran.mc(philly.test$sale_price.Error, 
                      spatialWeights.test, nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  plotTheme()

```

## Predictions by neighborhood

```{r}
philly.test %>%
as.data.frame() %>%
# Tract is not a good factor here
  group_by(census_tract) %>%
    summarize(meanPrediction = mean(sale_price.Predict),
              meanPrice = mean(sale_price)) %>%
      kable() %>% 
  kable_styling()

```